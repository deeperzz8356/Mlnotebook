{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2153904",
   "metadata": {},
   "source": [
    "Vector Calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48db163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6ac3c9c",
   "metadata": {},
   "source": [
    "LOSS  l(w)=||Ax-b||^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952f8e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "A=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "B=np.array([[9,8,7],[6,5,4],[3,2,1]])\n",
    "def loss_function(w):\n",
    "    return np.sum((A@w - B)**2)\n",
    "t=np.array([1,1,1])\n",
    "print(\"Loss:\", loss_function(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05c6df",
   "metadata": {},
   "source": [
    "GRadient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56d4beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of Loss: [[ 36 276 516]\n",
      " [ 36 336 636]\n",
      " [ 36 396 756]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "A=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "B=np.array([[9,8,7],[6,5,4],[3,2,1]])\n",
    "def gradient_loss_function(w):\n",
    "    return 2*A.T@(A@w - B)\n",
    "t=np.array([1,1,1])\n",
    "print(\"Gradient of Loss:\", gradient_loss_function(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498ad067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal w:\n",
      " [[-6.  -6.  -5.5]\n",
      " [-0.   1.   1. ]\n",
      " [ 5.   4.   3.5]]\n",
      "Loss at optimal w: 9.624103043696344e-28\n"
     ]
    }
   ],
   "source": [
    "#solve by putting gradient to zero\n",
    "A_T_A=np.dot(A.T,A)\n",
    "A_T_B=np.dot(A.T,B)\n",
    "w_optimal=np.linalg.solve(A_T_A,A_T_B)\n",
    "print(\"Optimal w:\\n\",w_optimal)\n",
    "print(\"Loss at optimal w:\", loss_function(w_optimal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "021ef10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of Loss: [[ 36 276 516]\n",
      " [ 36 336 636]\n",
      " [ 36 396 756]]\n",
      "w* (np.linalg.lstsq): [[-5.8333 -5.3333 -4.8333]\n",
      " [-0.3333 -0.3333 -0.3333]\n",
      " [ 5.1667  4.6667  4.1667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "A=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "B=np.array([[9,8,7],[6,5,4],[3,2,1]])\n",
    "def gradient_loss_function(w):\n",
    "    return 2*A.T@(A@w - B)\n",
    "t=np.array([1,1,1])\n",
    "print(\"Gradient of Loss:\", gradient_loss_function(t))\n",
    "w_np, *_ = np.linalg.lstsq(A, B, rcond=None)\n",
    "print(\"w* (np.linalg.lstsq):\", w_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6674d3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w* (np.linalg.lstsq): [[-5.8333 -5.3333 -4.8333]\n",
      " [-0.3333 -0.3333 -0.3333]\n",
      " [ 5.1667  4.6667  4.1667]]\n",
      "Loss after GD: 1.115260781677637e+69\n",
      "w after GD: [[-6.7718e+32 -5.3684e+32 -3.9650e+32]\n",
      " [-8.0804e+32 -6.4059e+32 -4.7313e+32]\n",
      " [-9.3891e+32 -7.4433e+32 -5.4975e+32]]\n",
      "Hessian:\n",
      " [[132 156 180]\n",
      " [156 186 216]\n",
      " [180 216 252]]\n",
      "Eigenvalues of Hessian: [567.7172   2.2828   0.    ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress=True)\n",
    "A=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "B=np.array([[9,8,7],[6,5,4],[3,2,1]])\n",
    "def gradient_loss_function(w):\n",
    "    return 2*A.T@(A@w - B)\n",
    "def loss_function(w):\n",
    "    return np.sum((A@w - B)**2)\n",
    "w_np, *_ = np.linalg.lstsq(A, B, rcond=None)\n",
    "print(\"w* (np.linalg.lstsq):\", w_np)\n",
    "\n",
    "lr = 0.01  # Define learning rate\n",
    "w = np.zeros((3,3))  # Initialize w as 3x3 array\n",
    "for i in range(50):\n",
    "    w = w  - lr * gradient_loss_function(w)\n",
    "print(\"Loss after GD:\", loss_function(w))\n",
    "print(\"w after GD:\", w)\n",
    "H = 2 * A.T @ A\n",
    "print(\"Hessian:\\n\", H)\n",
    "print(\"Eigenvalues of Hessian:\", np.linalg.eigvals(H))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
